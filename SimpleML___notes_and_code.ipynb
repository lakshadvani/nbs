{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**\n",
        "\n",
        "**Mechanics of Logistic Regression**\n",
        "\n",
        "Logistic Regression is a popular statistical method used for binary classification. It models the probability that an input **X** belongs to a particular category (usually 0 or 1).\n",
        "\n",
        "**Linear Combination of Features**\n",
        "\n",
        "z = WT . X + b\n",
        "\n",
        "**Sigmoid**\n",
        "\n",
        "Convert a linear value to 0 to 1\n",
        "\n",
        "Sigmoid (z) = 1/1+(e^(-z))\n",
        "\n",
        "The output of the sigmoid function is interpreted as the probability of the input belonging to the positive class (usually labeled as 1). If Ïƒ(z) is greater than 0.5, the input is classified as 1, otherwise as 0.\n",
        "\n",
        "Cost Function (Binary Cross-Entropy)\n",
        "Gradient Descent"
      ],
      "metadata": {
        "id": "Bk4JfsnBib6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression provides a probabilistic framework, offering not just classifications but the probabilities of predictions, which can be crucial for decision-making processes.\n",
        "Efficiency:\n",
        "\n",
        "It is computationally less intensive, making it a good choice for problems with not too many features.\n",
        "Interpretability:\n",
        "\n",
        "Logistic regression models are easy to interpret. The weights directly represent the importance of each feature for the prediction.\n",
        "Performance:\n",
        "\n",
        "It often performs well in binary classification tasks, especially when the classes are linearly separable."
      ],
      "metadata": {
        "id": "GElknQYFqJcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assumptions of Logistic Regression**\n",
        "\n",
        "\n",
        "Binary Output:\n",
        "\n",
        "-The dependent variable should be binary (0/1, True/False).\n",
        "\n",
        "-While logistic regression does not require the linear relationship between independent and dependent variables, it does require linear relationship between the log odds and the independent variables.\n",
        "\n",
        "\n",
        "-Logistic regression assumes little or no multicollinearity among the independent variables.\n",
        "\n",
        "-The observations should not be from repeated measurements or matched data.\n",
        "Large Sample Size:\n",
        "\n",
        "-Logistic regression requires a large sample size to predict accurately."
      ],
      "metadata": {
        "id": "-xo1nZQNqVls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Logistic Regression**\n",
        "\n",
        "Simplicity and Interpretability\n",
        "\n",
        "Efficient Computation\n",
        "\n",
        "Good Performance on Linearly Separable Data\n",
        "\n",
        "Probabilistic Approach\n",
        "\n",
        "Scalability\n",
        "\n",
        "Less Prone to Overfitting\n",
        "\n",
        "\n",
        "**Disadvantages of Logistic Regression**\n",
        "\n",
        "Assumption of Linearity\n",
        "\n",
        "Not Suitable for Large Number of Categorical Features\n",
        "\n",
        "Multicollinearity\n",
        "\n",
        "Limited to Binary or Multinomial Outcomes\n",
        "\n"
      ],
      "metadata": {
        "id": "iGD-1oyhso33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"Private method for the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the model to the data.\"\"\"\n",
        "        m, n = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient descent\n",
        "        for _ in range(self.iterations):\n",
        "            model = np.dot(X, self.weights) + self.bias\n",
        "            predictions = self._sigmoid(model)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / m) * np.dot(X.T, (predictions - y))\n",
        "            db = (1 / m) * np.sum(predictions - y)\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict binary labels for a set of inputs.\"\"\"\n",
        "        model = np.dot(X, self.weights) + self.bias\n",
        "        predictions = self._sigmoid(model)\n",
        "        return [1 if i > 0.5 else 0 for i in predictions]\n",
        "\n",
        "s\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTZ8XNuus5iy",
        "outputId": "4b3b72d9-e74c-40a5-c46e-e0264a396a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1, 1, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWlgsDtpzEHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**\n",
        "\n",
        "**Word2Vec**\n",
        "\n",
        "skip gram - predicts context ; cbow - predicts word\n",
        "Trained by simple NN no activation - ranodm weights - eventually aligns into the embedding\n",
        "\n",
        "fixed vocab ; no OOV\n",
        "\n",
        "**Fasttext**\n",
        "\n",
        "concept : Ngrams\n",
        "represent each word as character level n grams\n",
        "\n",
        "Word Representation: Each word is represented as the sum of these character n-gram vectors. This means that the word vector for \"apple\" is composed of the vectors of all its n-grams.\n",
        "\n",
        "\n",
        "\n",
        "Generating Embeddings for New Words: When encountering a word not seen during training, FastText computes its embedding by summing the vectors of its character n-grams. This allows the model to produce a meaningful representation for the OOV word based on its subword units.\n",
        "\n",
        "**Negative Sampling**\n",
        "\n",
        "Efficiency: The primary advantage of negative sampling is computational efficiency. The original Word2Vec model used a softmax function to calculate probabilities for every word in the vocabulary, which is computationally expensive. Negative sampling drastically reduces the number of output neurons that are updated in each training step.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HU0CArIzzE-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple embeddings coding implementations\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Example dataset: list of sentences (documents)\n",
        "documents = [\"the cat sat on the mat\", \"the dog barked at the cat\"]\n",
        "\n",
        "# Basic Preprocessing: tokenization (splitting text into words)\n",
        "tokenized_docs = [doc.split() for doc in documents]\n",
        "vocabulary = list(set(word for doc in tokenized_docs for word in doc))\n",
        "word_to_id = {word: i for i, word in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "7kKVeRRgzEJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(doc):\n",
        "    encoding = np.zeros(len(vocabulary))\n",
        "    for word in doc:\n",
        "        encoding[word_to_id[word]] = 1\n",
        "    return encoding\n",
        "\n",
        "one_hot_encoded_docs = [one_hot_encode(doc) for doc in tokenized_docs]"
      ],
      "metadata": {
        "id": "0YcPPgDq8rDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def term_frequency(doc):\n",
        "    tf = np.zeros(len(vocabulary))\n",
        "    for word in doc:\n",
        "        tf[word_to_id[word]] += 1\n",
        "    return tf / len(doc)\n",
        "\n",
        "tf_encoded_docs = [term_frequency(doc) for doc in tokenized_docs]"
      ],
      "metadata": {
        "id": "HTIjS5Ew8rFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"the dog barked\"\n",
        "]\n",
        "\n",
        "# Tokenize documents\n",
        "tokenized_documents = [doc.lower().split() for doc in documents]\n",
        "\n",
        "# Create a vocabulary\n",
        "vocabulary = list(set(word for doc in tokenized_documents for word in doc))\n",
        "\n",
        "# Calculate TF\n",
        "def calculate_tf(document, vocabulary):\n",
        "    tf_dict = dict.fromkeys(vocabulary, 0)\n",
        "    for word in document:\n",
        "        tf_dict[word] += 1\n",
        "    tf_dict = {word: count / len(document) for word, count in tf_dict.items()}\n",
        "    return list(tf_dict.values())\n",
        "\n",
        "tf = np.array([calculate_tf(doc, vocabulary) for doc in tokenized_documents])\n",
        "\n",
        "# Calculate IDF\n",
        "def calculate_idf(documents, vocabulary):\n",
        "    N = len(documents)\n",
        "    idf_dict = dict.fromkeys(vocabulary, 0)\n",
        "    for doc in documents:\n",
        "        for word in set(doc):\n",
        "            idf_dict[word] += 1\n",
        "    idf_dict = {word: np.log(N / float(count)) for word, count in idf_dict.items()}\n",
        "    return list(idf_dict.values())\n",
        "\n",
        "idf = np.array(calculate_idf(tokenized_documents, vocabulary))\n",
        "\n",
        "# Calculate TF-IDF\n",
        "tfidf = tf * idf\n",
        "\n",
        "print(tfidf)\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "( (no of times term t in doc d) / (number of unique t in doc d) * log (number of docs / no of docs with t)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "_bqROVfR8rGn",
        "outputId": "7c0587e7-dc05-4f3d-a9e7-e209c687cb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.07701635 0.         0.07701635 0.07701635 0.07701635 0.\n",
            "  0.07701635 0.         0.07701635]\n",
            " [0.         0.23104906 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n( (no of times term t in doc d) / (number of unique t in doc d) * log (number of docs / no of docs with t)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_bag_of_words(doc):\n",
        "    return np.where(term_frequency(doc) > 0, 1, 0)\n",
        "\n",
        "binary_bow_docs = [binary_bag_of_words(doc) for doc in tokenized_docs]"
      ],
      "metadata": {
        "id": "5Hv_HP4U8rKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_bag_of_words(doc):\n",
        "    bow = np.zeros(len(vocabulary))\n",
        "    for word in doc:\n",
        "        bow[word_to_id[word]] += 1\n",
        "    return bow\n",
        "\n",
        "count_bow_docs = [count_bag_of_words(doc) for doc in tokenized_docs]"
      ],
      "metadata": {
        "id": "qEJBFdvH8rL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_ngrams(words, n):\n",
        "    # Convert list of words to a NumPy array\n",
        "    words_array = np.array(words)\n",
        "\n",
        "    # Create an array of n-gram tuples\n",
        "    ngrams = np.lib.stride_tricks.sliding_window_view(words_array, window_shape=n)\n",
        "\n",
        "    # Convert n-grams to a list of tuples\n",
        "    ngrams = [tuple(ng) for ng in ngrams]\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "# Example usage\n",
        "words = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "n = 3  # For trigrams\n",
        "trigrams = generate_ngrams(words, n)\n",
        "\n",
        "print(trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2JAMqK68rNY",
        "outputId": "b8c1f10e-732d-4d7d-a81e-46bb0664b92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRoqAZLz8rPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mTLS11QVvcjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3FQ6wQb9Os3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**\n"
      ],
      "metadata": {
        "id": "lvGrNWT-ptQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "posterior = likelihood*prior / evidence\n",
        "\n",
        "The theorem is used to calculate the probability of a hypothesis (in this case, a class label) given some observed evidence (features in the data).\n",
        "\n",
        "p(H|E) = p(E|H)*P(H) / P(E)\n",
        "\n",
        "In the context of Naive Bayes for classification:\n",
        "H is the class label (e.g., \"spam\" or \"not spam\").\n",
        "E is the observed data (features).\n",
        "\n"
      ],
      "metadata": {
        "id": "7XD8_IUhptTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AUwrEPW4ptVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FGXCm45RptXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "# Features (binary features for simplicity)\n",
        "X = np.array([\n",
        "    [1, 0],  # Document 1 features\n",
        "    [1, 1],  # Document 2 features\n",
        "    [0, 1],  # Document 3 features\n",
        "    [0, 0],  # Document 4 features\n",
        "    [0, 1],  # Document 5 features\n",
        "    [1, 1]   # Document 6 features\n",
        "])\n",
        "\n",
        "# Labels (1 for positive class, 0 for negative class)\n",
        "y = np.array([1, 1, 1, 0, 0, 0])  # Corresponding labels for each document\n",
        "\n",
        "# Function to calculate the prior probability of each class\n",
        "def calculate_prior(y):\n",
        "    classes = np.unique(y)  # Unique classes in the labels\n",
        "    prior = np.zeros(len(classes))  # Prior probability for each class\n",
        "    for index, cls in enumerate(classes):\n",
        "        prior[index] = np.mean(y == cls)  # Fraction of documents of each class\n",
        "    return prior\n",
        "\n",
        "prior = calculate_prior(y)  # Calculate prior probabilities\n",
        "# Function to calculate likelihood of features given a class\n",
        "def calculate_likelihood(X, y):\n",
        "    classes = np.unique(y)  # Unique classes\n",
        "    n_features = X.shape[1]  # Number of features\n",
        "    # Likelihood of each feature value (0 or 1) given a class\n",
        "    likelihood = np.zeros((len(classes), n_features, 2))\n",
        "\n",
        "    for cls in classes:\n",
        "        for i in range(n_features):\n",
        "            # Probability of feature i being 0 given class cls\n",
        "            likelihood[cls, i, 0] = np.mean(X[y==cls, i] == 0)\n",
        "            # Probability of feature i being 1 given class cls\n",
        "            likelihood[cls, i, 1] = np.mean(X[y==cls, i] == 1)\n",
        "\n",
        "    return likelihood\n",
        "\n",
        "likelihood = calculate_likelihood(X, y)  # Calculate likelihood\n",
        "\n",
        "# Naive Bayes prediction function\n",
        "def naive_bayes_predict(X, prior, likelihood):\n",
        "    classes = np.unique(y)  # Unique classes\n",
        "    n_features = X.shape[1]  # Number of features\n",
        "    y_pred = np.zeros(X.shape[0])  # Array to hold predictions\n",
        "\n",
        "    for index, x in enumerate(X):\n",
        "        posteriors = np.zeros(len(classes))  # Posterior probability for each class\n",
        "\n",
        "        for cls in classes:\n",
        "            posterior = prior[cls]  # Start with the prior probability\n",
        "            for i in range(n_features):\n",
        "                # Multiply by likelihood of each feature\n",
        "                posterior *= likelihood[cls, i, x[i]]\n",
        "            posteriors[cls] = posterior  # Total posterior for this class\n",
        "\n",
        "        y_pred[index] = np.argmax(posteriors)  # Class with the highest posterior\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "predictions = naive_bayes_predict(X, prior, likelihood)  # Predict using the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFOIzNqxP372",
        "outputId": "05cfa179-5d07-44ce-bbc4-1f5adeda64fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "nlgCpGFGeUJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLUpv2M_rED9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reservoir Sampling\n",
        "Reservoir Sampling is a technique used to sample a fixed number of items from a data stream of unknown or very large size. It's particularly useful when the dataset is too large to fit into memory or when dealing with streaming data where the total number of items is not known in advance.\n",
        "\n",
        "How it Works:\n",
        "Initialize the Reservoir: Create a reservoir (array) with the size equal to the desired sample size. Fill the initial part of the reservoir with the first items of the stream.\n",
        "\n",
        "Process Each Item in the Stream: For each subsequent item in the stream (starting from the index that is one more than the reservoir size):\n",
        "\n",
        "With a certain probability, decide whether to include this item in the reservoir.\n",
        "If the item is selected to be included, it replaces a randomly chosen item in the reservoir.\n",
        "The probability of choosing an item is typically set so that each item in the stream has an equal chance of being included in the final sample.\n",
        "\n",
        "Key Features:\n",
        "Efficient for large or streaming datasets.\n",
        "Ensures a random sample without knowing the total size of the dataset.\n",
        "Every item in the stream has an equal chance of being included in the sample.\n",
        "Bootstrap Sampling\n",
        "Bootstrap Sampling is a statistical method used to estimate the distribution of a statistic (like mean, median, variance) by resampling with replacement from the data. It is a fundamental tool in the field of statistics and is widely used for assessing the variability of a statistic.\n",
        "\n",
        "How it Works:\n",
        "Draw Samples with Replacement: From the original dataset, draw a sample with replacement (meaning the same item can be chosen more than once) such that the sample size is equal to the original dataset size.\n",
        "\n",
        "Repeat the Process: Repeat this process many times, each time computing the statistic of interest from the bootstrap sample.\n",
        "\n",
        "Estimate the Distribution: Use the collection of computed statistics from all the bootstrap samples to estimate the distribution of the statistic. This can include calculating the standard error, confidence intervals, or other measures of statistical variability.\n",
        "\n",
        "Key Features:\n",
        "Does not require any assumptions about the distribution of the data.\n",
        "Suitable for small datasets, as it can help understand the variability of a statistic without needing new data.\n",
        "Widely used for non-parametric statistical inference.\n"
      ],
      "metadata": {
        "id": "zYrrqWYorEIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sampling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Random Sampling\n",
        "Random Sampling is straightforward with NumPy, using the np.random.choice function.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def random_sampling(data, sample_size):\n",
        "    sampled_data = np.random.choice(data, size=sample_size, replace=False)\n",
        "    return sampled_data\n",
        "\n",
        "# Example usage\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "sample_size = 5\n",
        "random_sample = random_sampling(data, sample_size)\n",
        "print(\"Random Sample:\", random_sample)\n",
        "\n",
        "\n",
        "Stratified Sampling\n",
        "For Stratified Sampling, you divide your data into different \"strata\" and then perform random sampling within each stratum.\n",
        "\n",
        "def stratified_sampling(data, labels, sample_size_per_stratum):\n",
        "    unique_labels = np.unique(labels)\n",
        "    sampled_data = []\n",
        "\n",
        "    for label in unique_labels:\n",
        "        stratum = data[labels == label]\n",
        "        stratum_sample = np.random.choice(stratum, size=sample_size_per_stratum, replace=False)\n",
        "        sampled_data.extend(stratum_sample)\n",
        "\n",
        "    return np.array(sampled_data)\n",
        "\n",
        "# Example usage\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "labels = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # Binary labels for simplicity\n",
        "sample_size_per_stratum = 2\n",
        "stratified_sample = stratified_sampling(data, labels, sample_size_per_stratum)\n",
        "print(\"Stratified Sample:\", stratified_sample)\n",
        "\n",
        "\n",
        "\n",
        "Reservoir Sampling\n",
        "Reservoir Sampling is used for sampling from a stream of data.\n",
        "\n",
        "def reservoir_sampling(stream, sample_size):\n",
        "    reservoir = np.zeros(sample_size)\n",
        "    for i, element in enumerate(stream):\n",
        "        if i < sample_size:\n",
        "            reservoir[i] = element\n",
        "        else:\n",
        "            j = np.random.randint(0, i+1)\n",
        "            if j < sample_size:\n",
        "                reservoir[j] = element\n",
        "    return reservoir\n",
        "\n",
        "# Example usage (assuming a stream of data)\n",
        "stream = np.arange(100)  # Stream of data\n",
        "sample_size = 10\n",
        "reservoir_sample = reservoir_sampling(stream, sample_size)\n",
        "print(\"Reservoir Sample:\", reservoir_sample)\n",
        "\n",
        "Bootstrap Sampling\n",
        "Bootstrap Sampling involves sampling with replacement.\n",
        "\n",
        "\n",
        "\n",
        "def bootstrap_sampling(data, sample_size):\n",
        "    sampled_data = np.random.choice(data, size=sample_size, replace=True)\n",
        "    return sampled_data\n",
        "\n",
        "# Example usage\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "sample_size = 10\n",
        "bootstrap_sample = bootstrap_sampling(data, sample_size)\n",
        "print(\"Bootstrap Sample:\", bootstrap_sample)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Tp-IB5VaeZfv",
        "outputId": "c9050275-bb11-4bfa-fdf5-146f789894d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-5d469b791543>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Random Sampling\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEccey8CnVCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ppz0efGli0uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RCI-F9lsi0wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnfQOlzoi0x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lWrrBjyxi05i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-j0MPVlTi07k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7W1oHhxFi09b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam Search**"
      ],
      "metadata": {
        "id": "I8SZnP5ci0_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def beam_search(decoder, beam_width, start_token, end_token):\n",
        "    \"\"\"\n",
        "    Perform beam search with a given decoder model.\n",
        "\n",
        "    :param decoder: A decoder model with a method get_next_tokens that takes a sequence and returns\n",
        "                    a list of tuples (next_token, probability).\n",
        "    :param beam_width: The number of beams to keep at each step.\n",
        "    :param start_token: The token that denotes the start of a sequence.\n",
        "    :param end_token: The token that denotes the end of a sequence.\n",
        "    :return: The best sequence found by beam search.\n",
        "    \"\"\"\n",
        "    beams = [([start_token], 0)]  # Initialize beams with the start token and zero score\n",
        "\n",
        "    while True:\n",
        "        new_beams = []\n",
        "\n",
        "        for seq, score in beams:\n",
        "            if seq[-1] == end_token:\n",
        "                # If the sequence ends with the end token, add it to the new beams\n",
        "                new_beams.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            # Get possible next tokens and their probabilities\n",
        "            next_tokens_with_probs = decoder.get_next_tokens(seq)\n",
        "\n",
        "            # Extend the sequence with these tokens and update the score\n",
        "            for token, prob in next_tokens_with_probs:\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score + np.log(prob)\n",
        "                new_beams.append((new_seq, new_score))\n",
        "\n",
        "        # Keep only the top beam_width sequences\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # Check if all beams end with the end token\n",
        "        if all(seq[-1] == end_token for seq, _ in beams):\n",
        "            break\n",
        "\n",
        "    # Return the sequence with the highest score\n",
        "    return max(beams, key=lambda x: x[1])[0]\n",
        "\n",
        "\n",
        "# Example Decoder Model\n",
        "class ExampleDecoder:\n",
        "    def __init__(self):\n",
        "        # This is a placeholder for an actual model. In a real scenario, this would be a trained model.\n",
        "        # The example here is simplistic and for illustration purposes only.\n",
        "        self.vocab = {'<start>': 0, '<end>': 1, 'hello': 2, 'world': 3}\n",
        "        self.probabilities = {\n",
        "            '<start>': [('hello', 0.6), ('<end>', 0.4)],\n",
        "            'hello': [('world', 0.8), ('<end>', 0.2)],\n",
        "            'world': [('<end>', 1.0)]\n",
        "        }\n",
        "\n",
        "    def get_next_tokens(self, sequence):\n",
        "        last_token = sequence[-1]\n",
        "        return self.probabilities.get(last_token, [('<end>', 1.0)])\n",
        "\n",
        "\n",
        "# Usage\n",
        "decoder = ExampleDecoder()\n",
        "beam_width = 2\n",
        "start_token = '<start>'\n",
        "end_token = '<end>'\n",
        "best_sequence = beam_search(decoder, beam_width, start_token, end_token)\n",
        "best_sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "xDg_Nh3ii3K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AW771i_Bi6os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "tokenizer - own\n",
        "simple lang model\n",
        "self attention\n",
        "'''"
      ],
      "metadata": {
        "id": "uik9-LVci6rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1rimport numpy as np\n",
        "\n",
        "class SelfAttentionWithCache:\n",
        "    def __init__(self, embedding_dim):\n",
        "        # Simplified: Using random matrices for query, key, and value weights\n",
        "        self.W_q = np.random.randn(embedding_dim, embedding_dim)\n",
        "        self.W_k = np.random.randn(embedding_dim, embedding_dim)\n",
        "        self.W_v = np.random.randn(embedding_dim, embedding_dim)\n",
        "        self.cache = {'K': [], 'V': []}  # Initialize cache for keys and values\n",
        "\n",
        "    def attention(self, Q, K, V):\n",
        "        # Calculate attention scores (scaled dot-product attention)\n",
        "        scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n",
        "        weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
        "        return np.dot(weights, V)\n",
        "\n",
        "    def update_cache(self, K, V):\n",
        "        # Append new keys and values to the cache\n",
        "        self.cache['K'].append(K)\n",
        "        self.cache['V'].append(V)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute Query, Key, and Value matrices\n",
        "        Q = np.dot(x, self.W_q)\n",
        "        K = np.dot(x, self.W_k)\n",
        "        V = np.dot(x, self.W_v)\n",
        "\n",
        "        # Update cache with the new Key and Value\n",
        "        self.update_cache(K, V)\n",
        "\n",
        "        # Use cached Keys and Values for attention calculation\n",
        "        cached_K = np.concatenate(self.cache['K'], axis=0)\n",
        "        cached_V = np.concatenate(self.cache['V'], axis=0)\n",
        "\n",
        "        # Calculate attention output\n",
        "        return self.attention(Q, cached_K, cached_V)\n",
        "\n",
        "# Example Usage\n",
        "embedding_dim = 64  # Example embedding dimension\n",
        "attention_layer = SelfAttentionWithCache(embedding_dim)\n",
        "\n",
        "# Example input (token embeddings)\n",
        "input_embedding = np.random.randn(1, embedding_dim)  # Single token for simplicity\n",
        "\n",
        "# Forward pass (assuming autoregressive generation, one token at a time)\n",
        "output_1 = attention_layer.forward(input_embedding)\n",
        "# ... process more tokens in a similar manner ...\n"
      ],
      "metadata": {
        "id": "unIB__aKRw77"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}